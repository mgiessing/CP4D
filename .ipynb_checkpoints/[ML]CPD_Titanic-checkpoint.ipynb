{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data extraction, inspection & preparation\n",
    "![title](img/DSP_Step1.png)\n",
    "![title](img/1_Extract.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, LabelEncoder, OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "df.groupby(['Survived','Sex']).size().unstack().plot(kind='bar', stacked=True, ax=axs[0], title='Survived vs Sex')\n",
    "df.groupby(['Survived','Pclass']).size().unstack().plot(kind='bar', stacked=True, ax=axs[1], title='Survived vs Pclass')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "df['Age'].plot(kind='kde', ax=axs[0], title='Age distribution')\n",
    "df['Fare'].plot(kind='hist', ax=axs[1], title='Fare distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model configuration, training & optimization\n",
    "![title](img/DSP_Step2.png)\n",
    "![title](img/2_Train.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop(columns=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also use sth. like LabelEncoder...\n",
    "df_new['Sex'] = pd.get_dummies(df_new['Sex'])\n",
    "\n",
    "#LabelEncoder\n",
    "#le = LabelEncoder()\n",
    "#le.fit_transform(df_new['Sex'])\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=df_new[['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']].to_numpy()\n",
    "X=df_new.drop(columns=['Survived']).to_numpy()\n",
    "y=df_new['Survived'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisiontree classifiers & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "scores = cross_val_score(clf, X_scaled, y, cv=10)\n",
    "print(\"CV mean: \", scores.mean())\n",
    "\n",
    "all_results['Decision Tree'] = [scores.mean(), clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "scores = cross_val_score(clf, X_scaled, y, cv=10)\n",
    "print(\"CV mean: \", scores.mean())\n",
    "\n",
    "all_results['Random Forest'] = [scores.mean(), clf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting classifier (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier, DMatrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#parameters = {'n_estimators': [x for x in range(5, 11, 5)], 'max_depth':[x for x in range(1,4)], 'learning_rate': [[round(x, 2) for x in np.arange(0.01, 0.11, 0.01)]]}\n",
    "parameters = {'n_estimators': [5], 'max_depth':[4], 'learning_rate': [0.01]}\n",
    "xgb = XGBClassifier()\n",
    "clf = GridSearchCV(xgb, parameters, cv=10)\n",
    "clf.fit(X_scaled, y)\n",
    "#print(clf.best_estimator_)\n",
    "print(\"CV mean: \", clf.best_score_)\n",
    "all_results['XG Boost'] = [clf.best_score_, clf]\n",
    "#xgb = XGBClassifier(n_estimators=10, max_depth=1, learning_rate=0.1, objective='binary:logistic')\n",
    "#cv_scores = cross_val_score(xgb, scaled, labels, cv=10)\n",
    "#print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "KERNELS = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "for kernel in KERNELS:\n",
    "    svc = svm.SVC(kernel=kernel, C=1.0)#.fit(x_train, y_train)\n",
    "    cv_scores = cross_val_score(svc, X_scaled, y, cv=10)\n",
    "    print(\"Kernel: {}, accuracy: {}\".format(kernel, cv_scores.mean()))\n",
    "    all_results[str('SVM-'+ kernel)] = [cv_scores.mean(),svc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest-neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "d = {}\n",
    "for i in range(1,50):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    cv_scores = cross_val_score(knn, X_scaled, y, cv=10)\n",
    "    d[str(i)]= [cv_scores.mean(), knn]\n",
    "    \n",
    "max_key = max(d, key=d.get)\n",
    "print(\"Best K: {} with accuracy: {}\".format(max_key, d[max_key][0]))\n",
    "all_results[str('KNN-'+max_key)] = [d[max_key][0], d[max_key][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "cv_scores = cross_val_score(lr, X_scaled, y, cv=10)\n",
    "print(cv_scores.mean())\n",
    "all_results['Logistic Regression'] = [cv_scores.mean(), lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_dim=6))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "est = KerasClassifier(build_fn=create_model, epochs=5)\n",
    "cv_scores = cross_val_score(est, X_scaled, y, cv=10)\n",
    "print(\"CV mean: \", cv_scores.mean())\n",
    "all_results['Neural Network'] = [cv_scores.mean(), est]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which algorithm performed best...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSee the full results in ascending order here:\\n\")\n",
    "for key, value in sorted(all_results.items(), reverse=False, key=lambda item: item[1]):\n",
    "    print(\"%s: %s\" % (key, value[0]))\n",
    "    clf_nm, clf = key, value[1]\n",
    "\n",
    "    \n",
    "from sklearn.pipeline import Pipeline\n",
    "# Make pipeline object\n",
    "print(\"\\nCreating pipeline object for best algorithm: {}\".format(clf_nm))\n",
    "pipe = Pipeline([('scaler', scaler), (clf_nm, clf)])\n",
    "model = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model deployment\n",
    "![title](img/DSP_Step3.png)\n",
    "![title](img/3_Deploy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training a model we can make it usable on our cluster in 3 steps:\n",
    "#### (Log in if necessary)\n",
    "#### 1.) Create a space or use an existing\n",
    "#### 2.) Create a model repository or use an existing\n",
    "#### 3.) Create a model deployment or use an existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Beim CP4Dv3.0.1 sollte eigentlich watson-machine-learning-client-V4 vorinstalliert sein und es sollte alles \"problemlos\" funktionieren!\n",
    "# Falls es dennoch Probleme geben sollte, mal \"vorsichtshalber\" alles deinstallieren und nur V4 installieren...\n",
    "# WICHTIG: Falls irgendwas uninstalled oder neuinstalled wurde den Jupyter Kernel neustarten!\n",
    "###\n",
    "\n",
    "#!pip uninstall watson-machine-learning-client-V4 -y\n",
    "#!pip uninstall watson-machine-learning-client -y\n",
    "#!pip uninstall ibm-watson-machine-learning -y\n",
    "#!pip install watson-machine-learning-client-V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1 - Curl:\n",
    "#!curl -k -X GET https://zen-cpd-zen.apps.edb-bde1.cecc.ihost.com/v1/preauth/validateAuth -u admin:password\n",
    "\n",
    "#Option 2 - Python requests:\n",
    "import requests, json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "### Folgende Variablen anpassen ###\n",
    "HOST_URL = 'https://zen-cpd-zen.apps.edb-efc5.cecc.ihost.com/v1/preauth/validateAuth'\n",
    "PROJECT_ID = 'a1e8bcdb-5a65-4c46-aa4d-f208d4f01b4a'\n",
    "### --------------------------- ###\n",
    "\n",
    "s = requests.Session()\n",
    "res = s.get(HOST_URL, auth=HTTPBasicAuth('admin', 'password'), verify=False)\n",
    "res = json.loads(res.text)\n",
    "token = res['accessToken']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "    \"token\": token,\n",
    "    \"instance_id\" : \"wml_local\",\n",
    "    \"url\"         : HOST_URL,\n",
    "    \"version\": \"3.0.1\"\n",
    "}\n",
    "\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "\n",
    "client.set.default_project(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List and/or create space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.spaces.get_details()['resources']:\n",
    "    print(\"Create resources!\")\n",
    "    space_details = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: \"dev_space2\"})\n",
    "    space_id = client.spaces.get_uid(space_details)\n",
    "else:\n",
    "    print(\"Resource vorhanden!\")\n",
    "    for sp in client.spaces.get_details()['resources']:\n",
    "        print(sp,\"\\n\")\n",
    "    print(\"Using '{}' as default space\".format(client.spaces.get_details()['resources'][0]['metadata']['name']))\n",
    "    space_id = client.spaces.get_details()['resources'][0]['metadata']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der zugehörigen Algorithmus ID ausgeben lassen...\n",
    "# client.software_specifications.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List or create model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Warnungen unterdrücken, da es sonst etwas verwirrend ist...\n",
    "\n",
    "if not client.repository.get_details()['models']['resources']:\n",
    "    print(\"Publish model...\")\n",
    "    software_spec_uid = client.software_specifications.get_uid_by_name(\"scikit-learn_0.22-py3.6\")\n",
    "    model_props = {\n",
    "        client.repository.ModelMetaNames.NAME: \"Titanic Survivor Prediction\",\n",
    "        client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.22\",\n",
    "        client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n",
    "        client.repository.ModelMetaNames.INPUT_DATA_SCHEMA: [{'id': '1',\n",
    "                                                                    'type': 'ndarray',\n",
    "                                                                     'fields': [{'name': 'Pclass', 'type': 'float'},\n",
    "                                                                                {'name': 'Sex', 'type': 'float'},\n",
    "                                                                                {'name': 'Age', 'type': 'float'},\n",
    "                                                                                {'name': 'Siblings/Spouses Aboard', 'type': 'float'},\n",
    "                                                                                {'name': 'Parents/Children Aboard', 'type': 'float'},\n",
    "                                                                                {'name': 'Fare', 'type': 'float'}]\n",
    "                                                                       }]\n",
    "    }\n",
    "    published_model = client.repository.store_model(model=model, pipeline=pipe, meta_props=model_props, training_data=x_train, training_target=y_train)\n",
    "else:\n",
    "    print(\"Model found!\")\n",
    "    print(\"Using default model {}\".format(client.repository.get_details()['models']['resources'][0]['metadata']['name']))\n",
    "    published_model = client.repository.get_details()['models']['resources'][0]\n",
    "\n",
    "# Optional: Delete by (gu)id\n",
    "#client.repository.delete('b6591272-ee24-40a7-841b-d7e8846277d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = client.repository.load(published_model_uid)\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.score(x_test, y_test)\n",
    "#test_predictions = loaded_model.predict(x_test).transform(x_test)\n",
    "#test_predictions.select('probability', 'predictedLabel').show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List or create deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.list()\n",
    "# Optional: Delete deployment\n",
    "# client.deployments.delete('70a57b43-3a4c-41ce-81e0-fdcf3a6116b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.deployments.get_details()['resources']:\n",
    "    print(\"Create deployment...\")\n",
    "    meta_props = {\n",
    "        client.deployments.ConfigurationMetaNames.NAME: \"Titanic Survivor Prediction\",\n",
    "        client.deployments.ConfigurationMetaNames.SPACE_UID: space_id,\n",
    "        client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "\n",
    "    created_deployment = client.deployments.create(artifact_uid=published_model_uid, meta_props=meta_props, name=\"Titanic Survivor Prediction\")\n",
    "else:\n",
    "    print(\"Deployment found!\")\n",
    "    print(\"Using default deployment: '{}' \".format(client.deployments.get_details()['resources'][0]['entity']['name']))\n",
    "    created_deployment = client.deployments.get_details()['resources'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\n",
    "deployment_id = created_deployment.get(\"metadata\").get(\"id\")\n",
    "print(f'Scoring endpoint is available at: {scoring_endpoint}')\n",
    "print(f'Deployment ID is: {deployment_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare scoring payload.\n",
    "job_payload = {\n",
    "    client.deployments.ScoringMetaNames.INPUT_DATA: [{\n",
    "        'values': [list(x_test[-1])]\n",
    "    }]\n",
    "}\n",
    "print(job_payload)\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "scaler.inverse_transform(x_test[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction and display the result.\n",
    "job_details = client.deployments.score(deployment_id, job_payload)\n",
    "print(job_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try our deployed model and see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's create two passengers:\n",
    "P1 which had a 3rd class ticket(3), was a male (0), 22 years old, had 1 sibling aboard, no parents or childrens aboard and paid 8.25 GBP for his ticket<br>\n",
    "--> Very similar datapoint to people who actually did NOT survive (0)...let's see<br><br>\n",
    "P2 which had a 1st class ticket(1), was a female (1), 38 years old, had 1 sibling aboard, no parents or childrens aboard and paid 70.5 GBP for her ticket<br>\n",
    "--> Very similar datapoint to people actually DID survive (1) ... let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Spielwiese:\n",
    "#(Survieved)->Pclass\tSex\tAge\tSiblings/Spouses Aboard\tParents/Children Aboard\tFare\n",
    "\n",
    "#0\t3\t0\t22.0\t1\t0\t7.2500\n",
    "#Hat nicht überlebt\n",
    "Passagier1 = [3, 0, 22., 1, 0 , 8.25]\n",
    "\n",
    "#1\t1\t1\t38.0\t1\t0\t71.2833\n",
    "#Hat überlebt\n",
    "Passagier2 = [1, 1, 38., 1, 0 , 70.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data using the scaler that was fit&transformed on the trainingsdata. Here we just need to transform data to our scaler using .transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komischerweise wird der Scaler im Pipeline Codesegment nochmal angepasst, daher muss der hier \"reinitialisiert\" werden\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "scaled_pass = scaler.transform([Passagier1, Passagier2])\n",
    "scaled_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the payload that we will send to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {client.deployments.ScoringMetaNames.INPUT_DATA: [{\n",
    "    'values': scaled_pass\n",
    "}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = client.deployments.score(deployment_id, payload)\n",
    "job_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the first passenger did not survive while the second (better class, but also paid more) did survive.<br>\n",
    "Here a little bit prettified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,num in enumerate(job_details['predictions'][0]['values']):\n",
    "    if 0 in num:\n",
    "        print(f'Passenger #{idx+1} would probably not survive')\n",
    "    elif 1 in num:\n",
    "        print(f'Passenger #{idx+1} would probably survive')\n",
    "    else:\n",
    "        print(\"That shouldn't have happened!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
